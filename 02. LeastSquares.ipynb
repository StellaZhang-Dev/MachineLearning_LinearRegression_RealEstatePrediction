{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Least Squares in Linear Regression\n",
    "\n",
    "In the previous section, we calculated price estimates directly using predefined coefficient values. While this approach is useful for prediction, it doesn’t leverage the full power of linear regression. The true strength of linear regression emerges when we flip the problem: rather than knowing the coefficients and predicting prices, we can use data to estimate the coefficients themselves. This allows us to determine how each feature influences the final price, which is the key to understanding relationships in data.\n",
    "\n",
    "## Why Can't We Always Get Perfect Predictions?\n",
    "\n",
    "In reality, it is nearly impossible to find coefficients that perfectly predict the prices for every data point. There are numerous reasons why this happens:\n",
    "\n",
    "- **External factors**: Prices are affected by factors outside of the model, such as market trends, location desirability, and the economic climate.\n",
    "\n",
    "- **Data noise**: Random variations in the data may introduce unpredictable fluctuations that are difficult for any model to capture.\n",
    "\n",
    "- **Confounding variables**: Some features may have hidden relationships that aren’t accounted for in the model, making predictions less reliable.\n",
    "\n",
    "- **Selection bias**: The data used to build the model may not represent the full population or all possible scenarios, leading to inaccuracies.\n",
    "\n",
    "For these reasons, linear regression models will usually make approximate predictions rather than exact ones. Therefore, it's important to critically assess how well the model reflects the true relationships in the data and understand its limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Coefficients with the Least Squares Method\n",
    "\n",
    "One of the most widely-used techniques for estimating the coefficients in a linear regression model is the least squares method. This method, developed by Adrien-Marie Legendre in the early 19th century, minimizes the sum of the squared differences between the actual observed values and the predicted values produced by the model.\n",
    "\n",
    "Given a dataset with known input features \\( X \\) and known output values \\( y \\), the goal is to find the coefficient vector \\( c \\) that minimizes the sum of squared errors (SSE):\n",
    "\n",
    "\\[\n",
    "SSE = \\sum (y_{\\text{actual}} - y_{\\text{predicted}})^2\n",
    "\\]\n",
    "\n",
    "The coefficients that minimize the SSE are those that make the model's predictions as close as possible to the true observed values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example: Finding the Best Coefficient Set\n",
    "\n",
    "To illustrate this concept, we will calculate the sum of squared errors for several different sets of coefficient values and identify which set provides the best fit for the data. This is a simplified example of the least squares method, where instead of finding the global optimum, we evaluate a fixed number of alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best set of coefficients is set 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data: Features (X) and actual prices (y)\n",
    "X = np.array([[66, 5, 15, 2, 500], \n",
    "              [21, 3, 50, 1, 100], \n",
    "              [120, 15, 5, 2, 1200]])\n",
    "\n",
    "y = np.array([250000, 60000, 525000])\n",
    "\n",
    "# Alternative sets of coefficient values\n",
    "c = np.array([[3000, 200, -50, 5000, 100], \n",
    "              [2000, -250, -100, 150, 250], \n",
    "              [3000, -100, -150, 0, 150]])\n",
    "\n",
    "def find_best(X, y, c):\n",
    "    smallest_error = np.inf  # Initialize with infinity to find minimum\n",
    "    best_index = -1  # To track the best set of coefficients\n",
    "    \n",
    "    for i, coeff in enumerate(c):\n",
    "        # Predict prices using current coefficient set\n",
    "        predictions = X @ coeff\n",
    "        \n",
    "        # Calculate sum of squared errors (SSE)\n",
    "        sse = np.sum((y - predictions) ** 2)\n",
    "        \n",
    "        # Update best index if current set has a smaller error\n",
    "        if sse < smallest_error:\n",
    "            smallest_error = sse\n",
    "            best_index = i\n",
    "    \n",
    "    print(\"The best set of coefficients is set %d\" % best_index)\n",
    "\n",
    "find_best(X, y, c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Least Squares Method Works\n",
    "\n",
    "The least squares method tries to find the best-fitting line (or hyperplane in higher dimensions) that minimizes the error between the actual data points and the predicted values. The key idea is to adjust the coefficients so that the sum of squared errors across all data points is as small as possible.\n",
    "\n",
    "In the example above, we are comparing three different sets of coefficients. For each set, we compute the predictions by multiplying the input features \\( X \\) by the coefficients \\( c \\), and then calculate the sum of squared errors for the difference between the actual prices and the predicted prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Fit\n",
    "\n",
    "To better understand how well each coefficient set fits the data, it is helpful to visualize the predictions and compare them to the actual prices. If the model is a good fit, the predicted values should lie close to the actual data points when plotted on a graph. By visualizing this relationship, we can determine whether a linear model is appropriate for the data at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The least squares method is a cornerstone technique in both statistics and machine learning for fitting linear models. By minimizing the sum of squared errors, it provides a simple yet effective way to estimate the coefficients that best explain the relationships in the data. While it may not always provide a perfect fit, especially in the presence of noise or bias, it is a powerful tool for understanding how features contribute to predictions.\n",
    "\n",
    "As we continue exploring linear regression, we will see how this method can be applied to more complex datasets and scenarios, extending beyond basic cabin price predictions to real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
