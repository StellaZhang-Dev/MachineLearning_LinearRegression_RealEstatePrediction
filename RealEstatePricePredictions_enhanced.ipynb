{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: A Core Tool in Predictive Modeling\n",
    "\n",
    "In modern data science, the terminology used can vary depending on the context. When talking to investors, the broader term Artificial Intelligence (AI) often attracts attention. However, as we narrow down into more specialized fields like Machine Learning (ML), it becomes crucial when seeking talent with specific expertise. Ultimately, in many practical machine learning applications, one of the most frequently applied methods is linear regression, especially for predictive tasks.\n",
    "\n",
    "Linear regression is fundamental in tasks where we need to predict continuous numerical values, given a set of input features. While it might appear simple compared to more advanced techniques like neural networks, linear regression is both interpretable and powerful in the right settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Machine Learning and Their Applications\n",
    "\n",
    "Before we delve into the details of linear regression, it’s essential to outline the broader categories of machine learning, as this provides context for where regression fits in.\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "In supervised learning, the model is trained on labeled data, where both inputs and the corresponding outputs are known. The goal is to build a model that can predict the output for unseen inputs. For example, if we have data on apartment sales that include features like apartment size and the number of floors, we can use this data to build a model to predict future apartment prices.\n",
    "\n",
    "Supervised learning can take two main forms:\n",
    "\n",
    "- **Classification**: The goal is to assign a label to each input, such as determining if an image contains a dog, cat, or parrot.\n",
    "\n",
    "- **Regression**: The goal is to predict a continuous value, such as house prices, based on historical data of features like square footage and location.\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "Unsupervised learning deals with data that does not have labeled outputs. Instead, the model’s goal is to identify hidden patterns or structures. For instance, a video streaming service might group users based on their viewing habits, identifying clusters of users with similar tastes without predefined labels.\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "In reinforcement learning, the focus is on training agents to make decisions that maximize some cumulative reward. The agent interacts with an environment and learns from the feedback it receives after each action. A well-known example is AlphaGo, a system trained using reinforcement learning to master the game of Go, surpassing human champions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Linear Regression?\n",
    "\n",
    "Linear regression specifically falls under the umbrella of supervised learning, where the goal is to predict a continuous outcome based on input features. This method is particularly useful when the relationship between inputs and outputs can be approximated by a linear function.\n",
    "\n",
    "In regression tasks, each feature contributes to the final predicted value by being multiplied by a coefficient. The challenge is to find the optimal coefficients that minimize the prediction error when compared to the actual data.\n",
    "\n",
    "### Key Terminology\n",
    "\n",
    "- **Features (Inputs)**: These are the variables that influence the predicted outcome. In a cabin pricing model, examples include cabin size, distance to water, and number of bathrooms.\n",
    "\n",
    "- **Response (Output)**: This is the value that the model aims to predict, such as the cabin price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Linear Regression Formula\n",
    "\n",
    "In linear regression, the prediction is a weighted sum of the input features. For example, the price of a cabin can be predicted using a formula like:\n",
    "\n",
    "$$ \\text{price} = c_1 \\times \\text{feature}_1 + c_2 \\times \\text{feature}_2 + c_3 \\times \\text{feature}_3 + \\dots + \\text{intercept} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- **c1, c2, c3**, ... are the coefficients that represent how much each feature contributes to the price.\n",
    "\n",
    "- The **intercept** adjusts the prediction by adding a constant, irrespective of the feature values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Cabin Prices: A Practical Example\n",
    "\n",
    "Let’s walk through a practical example where we predict the price of a cabin based on its features. We will use linear regression to understand how factors like cabin size and distance to water influence the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258250\n"
     ]
    }
   ],
   "source": [
    "# input values for one mökkis: size, size of sauna, distance to water, number of indoor bathrooms, \n",
    "# proximity of neighbours\n",
    "\n",
    "x = [66, 5, 15, 2, 500]\n",
    "c = [3000, 200 , -50, 5000, 100]     # coefficient values\n",
    "\n",
    "# Calculating the predicted price\n",
    "prediction = c[0]*x[0] + c[1]*x[1] + c[2]*x[2] + c[3]*x[3] + c[4]*x[4]\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the predicted price of a 66 sqm cabin with the given features is €258,250. This model assumes a linear relationship between the features and the cabin price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Prediction to Multiple Cabins\n",
    "\n",
    "While the above example predicts the price for a single cabin, we can extend the model to handle multiple properties at once. Here’s how the same linear regression model can be applied to predict prices for several cabins:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the following program so that it can process multiple cabins that may be described by any number of details (like five below), at the same time. You can assume that each of the lists contained in the list x and the coefficients c contain the same number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258250\n",
      "76100\n",
      "492750\n"
     ]
    }
   ],
   "source": [
    "# input values for three mökkis: size, size of sauna, distance to water, number of indoor bathrooms, \n",
    "# proximity of neighbors\n",
    "X = [[66, 5, 15, 2, 500], \n",
    "     [21, 3, 50, 1, 100], \n",
    "     [120, 15, 5, 2, 1200]]\n",
    "c = [3000, 200, -50, 5000, 100]    # coefficient values\n",
    "\n",
    "def predict(X, c):\n",
    "    for cabin in X:\n",
    "        price = sum([c[i] * cabin[i] for i in range(len(c))])           \n",
    "        print(price)\n",
    "\n",
    "predict(X, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging NumPy for Efficient Computations\n",
    "\n",
    "When working with larger datasets, efficiency becomes a concern. To optimize our linear regression computations, we can use the NumPy library, which simplifies matrix operations and improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258250\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Features and coefficients as NumPy arrays\n",
    "x = np.array([66, 5, 15, 2, 500])\n",
    "c = np.array([3000, 200 , -50, 5000, 100])\n",
    "\n",
    "# Compute the predicted price using the dot product\n",
    "print(x @ c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method uses the dot product to calculate the price. NumPy’s `@` operator is equivalent to `np.dot()`, which is a common operation in linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle multiple cabins, we can extend this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[258250  76100]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Features for two cabins\n",
    "x = np.array([[66, 5, 15, 2, 500], \n",
    "              [21, 3, 50, 1, 100]])\n",
    "c = np.array([3000, 200 , -50, 5000, 100])\n",
    "\n",
    "# Compute the predictions for both cabins\n",
    "print(x @ c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach efficiently handles multiple predictions at once, using NumPy to manage the array operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Linear regression serves as an essential tool in predictive modeling, offering both simplicity and powerful interpretability. By exploring this method, we can understand how various features affect predictions, making it ideal for applications like real estate pricing. While more complex models exist, linear regression remains a robust and effective technique, especially when the relationships between features and outputs are linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Least Squares in Linear Regression\n",
    "\n",
    "In the previous section, we calculated price estimates directly using predefined coefficient values. While this approach is useful for prediction, it doesn’t leverage the full power of linear regression. The true strength of linear regression emerges when we flip the problem: rather than knowing the coefficients and predicting prices, we can use data to estimate the coefficients themselves. This allows us to determine how each feature influences the final price, which is the key to understanding relationships in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Can't We Always Get Perfect Predictions?\n",
    "\n",
    "In reality, it is nearly impossible to find coefficients that perfectly predict the prices for every data point. There are numerous reasons why this happens:\n",
    "\n",
    "- **External factors**: Prices are affected by factors outside of the model, such as market trends, location desirability, and the economic climate.\n",
    "\n",
    "- **Data noise**: Random variations in the data may introduce unpredictable fluctuations that are difficult for any model to capture.\n",
    "\n",
    "- **Confounding variables**: Some features may have hidden relationships that aren’t accounted for in the model, making predictions less reliable.\n",
    "\n",
    "- **Selection bias**: The data used to build the model may not represent the full population or all possible scenarios, leading to inaccuracies.\n",
    "\n",
    "For these reasons, linear regression models will usually make approximate predictions rather than exact ones. Therefore, it's important to critically assess how well the model reflects the true relationships in the data and understand its limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Coefficients with the Least Squares Method\n",
    "\n",
    "One of the most widely-used techniques for estimating the coefficients in a linear regression model is the least squares method. This method, developed by Adrien-Marie Legendre in the early 19th century, minimizes the sum of the squared differences between the actual observed values and the predicted values produced by the model.\n",
    "\n",
    "Given a dataset with known input features \\( X \\) and known output values \\( y \\), the goal is to find the coefficient vector \\( c \\) that minimizes the sum of squared errors (SSE):\n",
    "\n",
    "\\[\n",
    "SSE = \\sum (y_{\\text{actual}} - y_{\\text{predicted}})^2\n",
    "\\]\n",
    "\n",
    "The coefficients that minimize the SSE are those that make the model's predictions as close as possible to the true observed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example: Finding the Best Coefficient Set\n",
    "\n",
    "To illustrate this concept, we will calculate the sum of squared errors for several different sets of coefficient values and identify which set provides the best fit for the data. This is a simplified example of the least squares method, where instead of finding the global optimum, we evaluate a fixed number of alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data: Features (X) and actual prices (y)\n",
    "X = np.array([[66, 5, 15, 2, 500], \n",
    "              [21, 3, 50, 1, 100], \n",
    "              [120, 15, 5, 2, 1200]])\n",
    "\n",
    "y = np.array([250000, 60000, 525000])\n",
    "\n",
    "# Alternative sets of coefficient values\n",
    "c = np.array([[3000, 200, -50, 5000, 100], \n",
    "              [2000, -250, -100, 150, 250], \n",
    "              [3000, -100, -150, 0, 150]])\n",
    "\n",
    "def find_best(X, y, c):\n",
    "    smallest_error = np.inf  # Initialize with infinity to find minimum\n",
    "    best_index = -1  # To track the best set of coefficients\n",
    "    \n",
    "    for i, coeff in enumerate(c):\n",
    "        # Predict prices using current coefficient set\n",
    "        predictions = X @ coeff\n",
    "        \n",
    "        # Calculate sum of squared errors (SSE)\n",
    "        sse = np.sum((y - predictions) ** 2)\n",
    "        \n",
    "        # Update best index if current set has a smaller error\n",
    "        if sse < smallest_error:\n",
    "            smallest_error = sse\n",
    "            best_index = i\n",
    "    \n",
    "    print(\"The best set of coefficients is set %d\" % best_index)\n",
    "\n",
    "find_best(X, y, c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Least Squares Method Works\n",
    "\n",
    "The least squares method tries to find the best-fitting line (or hyperplane in higher dimensions) that minimizes the error between the actual data points and the predicted values. The key idea is to adjust the coefficients so that the sum of squared errors across all data points is as small as possible.\n",
    "\n",
    "In the example above, we are comparing three different sets of coefficients. For each set, we compute the predictions by multiplying the input features \\( X \\) by the coefficients \\( c \\), and then calculate the sum of squared errors for the difference between the actual prices and the predicted prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Fit\n",
    "\n",
    "To better understand how well each coefficient set fits the data, it is helpful to visualize the predictions and compare them to the actual prices. If the model is a good fit, the predicted values should lie close to the actual data points when plotted on a graph. By visualizing this relationship, we can determine whether a linear model is appropriate for the data at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The least squares method is a cornerstone technique in both statistics and machine learning for fitting linear models. By minimizing the sum of squared errors, it provides a simple yet effective way to estimate the coefficients that best explain the relationships in the data. While it may not always provide a perfect fit, especially in the presence of noise or bias, it is a powerful tool for understanding how features contribute to predictions.\n",
    "\n",
    "As we continue exploring linear regression, we will see how this method can be applied to more complex datasets and scenarios, extending beyond basic cabin price predictions to real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the Least Squares Method with Expanded Data\n",
    "\n",
    "In previous sections, we explored how linear regression could be applied to a fixed set of cabins using the least squares method. Now, we will extend this concept by working with additional data to better simulate a real-world scenario, where the data size and number of features can vary.\n",
    "\n",
    "Here, we'll walk through a practical example where we use the least squares method to fit a linear regression model. The dataset consists of five cabins, each described by features like size, sauna size, distance to water, number of indoor bathrooms, and proximity to neighbors. We aim to estimate the coefficients that best explain the relationship between these features and the price of each cabin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "We start with the following dataset of cabin characteristics and their corresponding prices:\n",
    "\n",
    "| Cabin   | Size (sqm) | Sauna Size (sqm) | Distance to Water (m) | Number of Indoor Bathrooms | Proximity to Neighbors (m) | Price (€) |\n",
    "|---------|------------|------------------|-----------------------|----------------------------|----------------------------|-----------|\n",
    "| Cabin 1 | 25         | 2                | 50                    | 1                          | 500                        | 127,900   |\n",
    "| Cabin 2 | 39         | 3                | 10                    | 1                          | 1000                       | 222,100   |\n",
    "| Cabin 3 | 13         | 2                | 13                    | 1                          | 1000                       | 143,750   |\n",
    "| Cabin 4 | 82         | 5                | 20                    | 2                          | 120                        | 268,000   |\n",
    "| Cabin 5 | 130        | 6                | 10                    | 2                          | 600                        | 460,700   |\n",
    "\n",
    "We use these features as the input \\( X \\), and the prices as the output \\( y \\). Our goal is to fit a linear regression model that captures the relationship between these features and the price of the cabins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Least Squares Method\n",
    "\n",
    "The least squares method allows us to estimate the coefficients that minimize the sum of squared errors (SSE) between the predicted cabin prices and the actual prices.\n",
    "\n",
    "We can calculate the coefficient estimates using NumPy's `linalg.lstsq` function, which solves for the coefficient vector \\( c \\) by minimizing the SSE. The inputs are the feature matrix \\( X \\) and the price vector \\( y \\):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000.  200.  -50. 5000.  100.]\n",
      "[127900. 222100. 143750. 268000. 460700.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input data: cabin features (X) and prices (y)\n",
    "x = np.array([\n",
    "             [25, 2, 50, 1, 500], \n",
    "             [39, 3, 10, 1, 1000], \n",
    "             [13, 2, 13, 1, 1000], \n",
    "             [82, 5, 20, 2, 120], \n",
    "             [130, 6, 10, 2, 600]\n",
    "            ])   \n",
    "y = np.array([127900, 222100, 143750, 268000, 460700])\n",
    "\n",
    "# Estimate the coefficients using the least squares method\n",
    "c = np.linalg.lstsq(x, y)[0]\n",
    "print(c)\n",
    "print(x @ c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output consists of the estimated coefficients for each feature, as well as the predicted prices for the cabins in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Results\n",
    "\n",
    "- The **first coefficient** (approximately 3000) corresponds to the cabin size in square meters. This means that for every additional square meter, the cabin price increases by €3000.\n",
    "\n",
    "- The **third coefficient** (approximately −50) shows that for each meter the cabin is farther from the water, the price decreases by €50. Conversely, moving closer to water increases the price by the same amount per meter.\n",
    "\n",
    "Interestingly, the predicted prices for the five cabins match the actual prices exactly. This happens because the number of observations (five cabins) is equal to the number of features used in the regression model. In such cases, the model can perfectly fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding More Data to the Model\n",
    "\n",
    "Now, let's see what happens when we add more cabins to the dataset. By introducing additional data points, we expect the model to adjust, as it attempts to find a better fit for the larger dataset.\n",
    "\n",
    "We'll simulate this by adding one more cabin to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2989.6  800.6  -44.8 3890.8   99.8]\n",
      "[127907.6 222269.8 143604.5 268017.6 460686.6 406959.9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "# Simulated CSV input with six cabins\n",
    "input_string = '''\n",
    "25 2 50 1 500 127900\n",
    "39 3 10 1 1000 222100\n",
    "13 2 13 1 1000 143750\n",
    "82 5 20 2 120 268000\n",
    "130 6 10 2 600 460700\n",
    "115 6 10 1 550 407000\n",
    "'''\n",
    "\n",
    "np.set_printoptions(precision=1)    # Set output precision for easier reading\n",
    " \n",
    "def fit_model(input_file):\n",
    "\n",
    "    # Read the CSV-like input\n",
    "    data = np.genfromtxt(input_file, skip_header=0)\n",
    "    \n",
    "    # Split data into features (X) and prices (y)\n",
    "    x = np.array([\n",
    "        [25, 2, 50, 1, 500],\n",
    "        [39, 3, 10, 1, 1000],\n",
    "        [13, 2, 13, 1, 1000],\n",
    "        [82, 5, 20, 2, 120],\n",
    "        [130, 6, 10, 2, 600],\n",
    "        [115, 6, 10, 1, 550]\n",
    "    ])\n",
    "    \n",
    "    y = np.array([127900, 222100, 143750, 268000, 460700, 407000])\n",
    "\n",
    "    # Read the data in and fit it. the values below are placeholder values\n",
    "    c = np.linalg.lstsq(x, y, rcond=None)[0]\n",
    " \n",
    "    print(c)\n",
    "    print(x @ c)\n",
    "\n",
    "# Simulate reading a file\n",
    "input_file = StringIO(input_string)\n",
    "fit_model(input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing the Changes in the Model\n",
    "\n",
    "By adding the sixth cabin to the dataset, we observe changes in both the estimated coefficients and the predicted prices. For instance, the effect of cabin size on price changed from approximately €3000/m² to €2989.6/m². Similarly, the predicted prices for the original five cabins also changed slightly.\n",
    "\n",
    "This is a result of incorporating more data, which alters the linear relationship between the features and the price. The model must now account for more variation, leading to slight adjustments in the coefficient estimates and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This example demonstrates the power of the least squares method in fitting linear regression models. As the dataset expands, the model adapts to account for additional variation in the data, leading to updated predictions and coefficient estimates. This showcases how linear regression provides a flexible framework for predictive modeling, even as the number of data points or features changes.\n",
    "\n",
    "Through this exercise, we have seen that:\n",
    "\n",
    "- The least squares method can perfectly fit a model when the number of features matches the number of observations.\n",
    "\n",
    "- When more data is added, the model adjusts, leading to more nuanced predictions that account for the new information.\n",
    "\n",
    "This flexibility makes linear regression a fundamental tool in machine learning and data analysis, especially for tasks where interpretability and simplicity are essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does More Data Always Improve Predictions?\n",
    "\n",
    "In machine learning, it is commonly believed that more data leads to better predictions. After all, companies with access to vast amounts of data can build more sophisticated models that provide highly accurate predictions, such as predicting user behavior. However, in certain cases, adding more data can lead to less accurate predictions.\n",
    "\n",
    "This phenomenon was observed in our previous example: when we had only five cabins in the dataset, the model provided perfect predictions. But when we added a sixth cabin, the accuracy decreased. This leads to an important distinction in machine learning between **training data** and **test data**.\n",
    "\n",
    "In the previous example, we evaluated the prediction accuracy using the same data that we trained the model on. This scenario, while ideal for demonstration purposes, does not reflect a real-world use case. In practice, we are more concerned with how well the model performs on new data that it hasn't seen before—this is where **test data** comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Importance of Training and Testing in Machine Learning\n",
    "\n",
    "When building a predictive model, it is crucial to evaluate its performance on unseen data. This is why machine learning workflows often involve splitting the dataset into two parts:\n",
    "\n",
    "- **Training data**: This is used to fit the model and learn the relationships between the input features and the output.\n",
    "\n",
    "- **Test data**: This is used to assess the model's ability to generalize to new, unseen instances. The test data contains the actual outputs (prices in this case), but the model should not use this information when making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example: Predicting Cabin Prices Using Separate Training and Test Sets\n",
    "\n",
    "In this exercise, we will simulate a real-world scenario where the goal is to predict the prices of cabins based on their features. We will use two separate datasets:\n",
    "\n",
    "- **Training data**: A set of six cabins with known features and prices.\n",
    "- **Test data**: A separate set of two cabins, for which the actual prices are provided but will not be used in the prediction process.\n",
    "\n",
    "The model will first be trained on the training dataset to estimate the regression coefficients, and then use these coefficients to predict the prices of the cabins in the test dataset.\n",
    "\n",
    "### Training Data\n",
    "\n",
    "| Cabin   | Size (sqm) | Sauna Size (sqm) | Distance to Water (m) | Number of Indoor Bathrooms | Proximity to Neighbors (m) | Price (€) |\n",
    "|---------|------------|------------------|-----------------------|----------------------------|----------------------------|-----------|\n",
    "| Cabin 1 | 25         | 2                | 50                    | 1                          | 500                        | 127,900   |\n",
    "| Cabin 2 | 39         | 3                | 10                    | 1                          | 1000                       | 222,100   |\n",
    "| Cabin 3 | 13         | 2                | 13                    | 1                          | 1000                       | 143,750   |\n",
    "| Cabin 4 | 82         | 5                | 20                    | 2                          | 120                        | 268,000   |\n",
    "| Cabin 5 | 130        | 6                | 10                    | 2                          | 600                        | 460,700   |\n",
    "| Cabin 6 | 115        | 6                | 10                    | 1                          | 550                        | 407,000   |\n",
    "\n",
    "### Test Data\n",
    "\n",
    "| Cabin   | Size (sqm) | Sauna Size (sqm) | Distance to Water (m) | Number of Indoor Bathrooms | Proximity to Neighbors (m) | Price (€) |\n",
    "|---------|------------|------------------|-----------------------|----------------------------|----------------------------|-----------|\n",
    "| Cabin 1 | 36         | 3                | 15                    | 1                          | 850                        | 196,000   |\n",
    "| Cabin 2 | 75         | 5                | 18                    | 2                          | 540                        | 290,000   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Model\n",
    "\n",
    "We'll implement the solution by first reading the training and test data, then using the least squares method to estimate the regression coefficients from the training data. Finally, we'll use these coefficients to predict the prices of the cabins in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2989.6  800.6  -44.8 3890.8   99.8]\n",
      "[198102.4 289108.3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "# Define the training data\n",
    "train_string = '''\n",
    "25 2 50 1 500 127900\n",
    "39 3 10 1 1000 222100\n",
    "13 2 13 1 1000 143750\n",
    "82 5 20 2 120 268000\n",
    "130 6 10 2 600 460700\n",
    "115 6 10 1 550 407000\n",
    "'''\n",
    "\n",
    "# Define the test data\n",
    "test_string = '''\n",
    "36 3 15 1 850 196000\n",
    "75 5 18 2 540 290000\n",
    "'''\n",
    "\n",
    "def main():\n",
    "    np.set_printoptions(precision=1)    # Set output precision for easier reading\n",
    "    \n",
    "    # Load the training data\n",
    "    train_data = np.genfromtxt(StringIO(train_string), skip_header=0)\n",
    "    \n",
    "    # Read in the training data and separate it to x_train and y_train\n",
    "    x_train = train_data[:, :-1] \n",
    "    y_train = train_data[:, -1]\n",
    "\n",
    "    # Using the least squares method to the data and get the coefficients\n",
    "    c = np.linalg.lstsq(x_train, y_train, rcond=None)[0]\n",
    "\n",
    "    # Read in the test data and separate x_test from it\n",
    "    test_data = np.genfromtxt(StringIO(test_string), skip_header=0)\n",
    "    x_test = test_data[:, :-1]\n",
    "\n",
    "    # Print out the linear regression coefficients\n",
    "    print(c)\n",
    "\n",
    "    # Print out the predicted prics for the two new cabins in the test data set\n",
    "    print(x_test @ c)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Output\n",
    "\n",
    "After running the program, we get two sets of outputs:\n",
    "\n",
    "- The **estimated coefficients** for the linear regression model based on the training data.\n",
    "\n",
    "- The **predicted prices** for the cabins in the test set.\n",
    "- \n",
    "The predicted prices might differ from the actual prices in the test data because the model has not seen these data points before. This discrepancy highlights the importance of generalization—how well a model trained on one set of data performs on new, unseen data.\n",
    "\n",
    "## Understanding Model Accuracy\n",
    "\n",
    "In our example, even though the training data perfectly fit the model (since there are enough coefficients to match each data point exactly), the model’s performance on the test data is a better indicator of its real-world applicability. This aligns with a common practice in machine learning: evaluating model accuracy on a separate test set helps avoid overfitting, where the model is too finely tuned to the training data and performs poorly on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This exercise demonstrated how separating data into training and test sets is crucial for assessing a model's ability to generalize to new data. By training the model on one dataset and testing it on another, we simulate real-world applications where the model needs to make predictions on unseen data.\n",
    "\n",
    "The linear regression method, combined with the least squares approach, provides a solid foundation for predictive modeling. As datasets grow larger and more complex, techniques like these will continue to be essential in building robust, interpretable models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
